# -*- coding: utf-8 -*-
"""scrappdf

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/190DD8agPhQv_bmym3gybxK5pEmILlws3
"""

import asyncio
import aiohttp
import boto3
from urllib.parse import urlparse
from pathlib import Path
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig

# Configuration
WEBSITE_URL = "https://news.ycombinator.com"
SESSION_ID = "scrape_session"
HEADLESS_MODE = True
PAGE_TIMEOUT = 60000
SCROLL_TIMEOUT = 30000
SCROLL_DELAY = 2.0
WAIT_FOR_ELEMENT = "css:body"

# S3 Configuration (set these as environment variables or replace with your values)
S3_BUCKET_NAME = "your-bucket-name"
S3_REGION = "us-east-1"

# File extensions that should be downloaded instead of scraped
DOWNLOADABLE_EXTENSIONS = {
    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx',
    '.zip', '.rar', '.7z', '.tar', '.gz', '.bz2',
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff',
    '.mp4', '.avi', '.mov', '.wmv', '.flv', '.webm',
    '.mp3', '.wav', '.flac', '.aac', '.ogg',
    '.txt', '.csv', '.json', '.xml', '.sql'
}

def is_downloadable_file(url: str) -> bool:
    """Check if URL points to a downloadable file based on extension."""
    parsed_url = urlparse(url)
    path = parsed_url.path.lower()
    return any(path.endswith(ext) for ext in DOWNLOADABLE_EXTENSIONS)

async def check_content_type(url: str) -> tuple[bool, str]:
    """Check if URL points to a downloadable file based on Content-Type header."""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.head(url, allow_redirects=True) as response:
                content_type = response.headers.get('content-type', '').lower()

                # Check for non-HTML content types that should be downloaded
                downloadable_types = [
                    'application/pdf',
                    'application/msword',
                    'application/vnd.openxmlformats-officedocument',
                    'application/vnd.ms-excel',
                    'application/vnd.ms-powerpoint',
                    'application/zip',
                    'application/octet-stream',
                    'image/',
                    'video/',
                    'audio/',
                    'application/json',
                    'text/csv',
                    'application/xml'
                ]

                is_downloadable = any(dtype in content_type for dtype in downloadable_types)
                return is_downloadable, content_type
    except Exception as e:
        print(f"Warning: Could not check content type for {url}: {e}")
        return False, ""

async def download_file_to_s3(url: str, s3_key: str = None) -> dict:
    """Download file from URL and upload to S3."""
    try:
        # Generate S3 key if not provided
        if not s3_key:
            parsed_url = urlparse(url)
            filename = Path(parsed_url.path).name or "downloaded_file"
            s3_key = f"downloads/{filename}"

        # Download file
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status != 200:
                    raise RuntimeError(f"Failed to download file: HTTP {response.status}")

                file_content = await response.read()
                content_type = response.headers.get('content-type', 'application/octet-stream')

        # Upload to S3
        s3_client = boto3.client('s3', region_name=S3_REGION)
        s3_client.put_object(
            Bucket=S3_BUCKET_NAME,
            Key=s3_key,
            Body=file_content,
            ContentType=content_type
        )

        s3_url = f"https://{S3_BUCKET_NAME}.s3.{S3_REGION}.amazonaws.com/{s3_key}"

        return {
            'success': True,
            'file_type': 'download',
            'original_url': url,
            's3_key': s3_key,
            's3_url': s3_url,
            'file_size': len(file_content),
            'content_type': content_type
        }

    except Exception as e:
        return {
            'success': False,
            'file_type': 'download',
            'original_url': url,
            'error': str(e)
        }

async def process_url(url: str, session_id: str = SESSION_ID) -> dict:
    """
    Main function to process a URL - either scrape HTML or download file.
    Returns a dict with the result and metadata.
    """
    # First check if it's a downloadable file by extension
    if is_downloadable_file(url):
        print(f"Detected downloadable file by extension: {url}")
        return await download_file_to_s3(url)

    # Check content type via HEAD request
    is_downloadable, content_type = await check_content_type(url)
    if is_downloadable:
        print(f"Detected downloadable file by content-type ({content_type}): {url}")
        return await download_file_to_s3(url)

    # If not a downloadable file, proceed with web scraping
    print(f"üåê Processing as webpage: {url}")
    try:
        # Navigate to page
        crawler, result = await navigate_to_page(url, session_id)

        # Scroll to load all content
        await scroll_to_bottom(crawler, result.url, session_id)

        # Get final HTML
        html = await get_outer_html(crawler, result.url, session_id)

        await crawler.__aexit__(None, None, None)

        return {
            'success': True,
            'file_type': 'html',
            'original_url': url,
            'final_url': result.url,
            'status_code': result.status_code,
            'html': html,
            'html_length': len(html)
        }

    except Exception as e:
        return {
            'success': False,
            'file_type': 'html',
            'original_url': url,
            'error': str(e)
        }

# Existing functions (unchanged)
async def navigate_to_page(url: str, session_id: str = SESSION_ID):
    """Navigate to a webpage and return crawler + result."""
    browser_cfg = BrowserConfig(browser_mode="playwright", headless=HEADLESS_MODE)
    run_cfg = CrawlerRunConfig(
        wait_for=WAIT_FOR_ELEMENT,
        page_timeout=PAGE_TIMEOUT,
        session_id=session_id
    )

    crawler = AsyncWebCrawler(config=browser_cfg)
    await crawler.__aenter__()

    result = await crawler.arun(url=url, config=run_cfg)

    if not result.success:
        await crawler.__aexit__(None, None, None)
        raise RuntimeError(f"Navigation failed: {result.error_message}")

    return crawler, result

async def scroll_to_bottom(crawler, url: str, session_id: str = SESSION_ID):
    """Scroll to bottom to load all dynamic content."""
    scroll_cfg = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);",
        js_only=True,
        session_id=session_id,
        delay_before_return_html=SCROLL_DELAY,
        page_timeout=SCROLL_TIMEOUT
    )

    result = await crawler.arun(url=url, config=scroll_cfg)

    if not result.success:
        raise RuntimeError(f"Scroll failed: {result.error_message}")

    return result

async def get_outer_html(crawler, url: str, session_id: str = SESSION_ID):
    """Get the complete HTML after all content is loaded."""
    html_cfg = CrawlerRunConfig(
        js_only=True,
        session_id=session_id,
        page_timeout=SCROLL_TIMEOUT
    )

    result = await crawler.arun(url=url, config=html_cfg)

    if not result.success:
        raise RuntimeError(f"HTML retrieval failed: {result.error_message}")

    return result.html

# Example usage
if __name__ == "__main__":
    async def test():
        # Test with different types of URLs
        test_urls = [
            "https://news.ycombinator.com",  # Regular webpage
            "https://example.com/document.pdf",  # PDF file (example)
        ]

        for url in test_urls:
            print(f"\n{'='*60}")
            print(f"Processing: {url}")
            print('='*60)

            result = await process_url(url)

            if result['success']:
                if result['file_type'] == 'html':
                    print(f"‚úì Scraped webpage: {result['final_url']}")
                    print(f"  Status: {result['status_code']}")
                    print(f"  HTML length: {result['html_length']} chars")
                    print(f"  First 200 chars: {result['html'][:200]}...")
                elif result['file_type'] == 'download':
                    print(f"‚úì Downloaded file to S3:")
                    print(f"  S3 URL: {result['s3_url']}")
                    print(f"  File size: {result['file_size']} bytes")
                    print(f"  Content type: {result['content_type']}")
            else:
                print(f"‚úó Failed: {result['error']}")

    # Run the test
    asyncio.run(test())